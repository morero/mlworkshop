{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ae4626311db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlinkedin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinkedin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.pickle import read_pickle\n",
    "import nltk\n",
    "from nltk import stem, tokenize\n",
    "from linkedin import linkedin\n",
    "import numpy as np\n",
    "import sklearn.cross_validation as cv\n",
    "from sklearn import linear_model, metrics, ensemble, grid_search, lda, neighbors, tree\n",
    "import math\n",
    "import matplotlib.pylab as py\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.feature_selection as fs\n",
    "import xlwt\n",
    "\n",
    "# Linkedin oauth details:\n",
    "LINKEDIN_CONSUMER_KEY       = ''\n",
    "LINKEDIN_CONSUMER_SECRET    = ''\n",
    "\n",
    "# For LinkedIn API calls:\n",
    "LINKEDIN_OAUTH_USER_TOKEN = ''\n",
    "LINKEDIN_OAUTH_USER_SECRET = ''\n",
    "RETURN_URL = 'http://localhost:8000'\n",
    "\n",
    "stemmer = stem.PorterStemmer()\n",
    "\n",
    "def estimate_seniority(job_position):\n",
    "    # Estimates the seniority of a job\n",
    "    # based on key words in the job position text\n",
    "    \n",
    "    # Input\n",
    "    # job_position: input text\n",
    "    \n",
    "    # Output\n",
    "    # seniority: 'junior', 'default', or 'senior'\n",
    "    \n",
    "    seniority = 'default'\n",
    "    jobtitlewords = job_position.lower().split()\n",
    "            \n",
    "    # ignore internships\n",
    "    if (('intern' in jobtitlewords)\n",
    "        or ('internship' in jobtitlewords)):\n",
    "        return 'INTERN'\n",
    "            \n",
    "    senior_words = ['sr',\n",
    "                    'senior',\n",
    "                    'lead',\n",
    "                    'principal',\n",
    "                    'director',\n",
    "                    'manager',\n",
    "                    'cto',\n",
    "                    'chief',\n",
    "                    'vp',\n",
    "                    'head'\n",
    "                    ]\n",
    "            \n",
    "    junior_words = ['jr',\n",
    "                    'junior',\n",
    "                    'associate',\n",
    "                    'assistant'\n",
    "                    ]\n",
    "            \n",
    "    for titleword in jobtitlewords:       \n",
    "                 \n",
    "        titleword = titleword.replace(',', '')\n",
    "        titleword = titleword.replace('.', '')  \n",
    "                      \n",
    "        if titleword in senior_words:\n",
    "            seniority = 'senior'\n",
    "            break\n",
    "        elif titleword in junior_words:\n",
    "            seniority = 'junior'\n",
    "            break\n",
    "        \n",
    "    return seniority\n",
    "\n",
    "def get_word_tokenize(text):\n",
    "    # Tokenize a string of input text\n",
    "    \n",
    "    # Input\n",
    "    # text: input text\n",
    "    \n",
    "    # Output\n",
    "    # list of tokenized words \n",
    "       \n",
    "    sentences = [s for s in nltk.sent_tokenize(text)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "    return [w.lower() for sentence in normalized_sentences for w in nltk.word_tokenize(sentence)]  \n",
    "\n",
    "def get_top_n_words(words, n, stopwords): \n",
    "    # Return the top n most frequent words from a tokenized list of words, using the input stopwords\n",
    "    \n",
    "    # Input\n",
    "    # words: tokenized words\n",
    "    # n: Top N words to return\n",
    "    # stopwords: List of stopwords\n",
    "    \n",
    "    # Output\n",
    "    # top_n_words: Top N most frequent words\n",
    "      \n",
    "    fdist = nltk.FreqDist(words)\n",
    "    top_n_words = [w[0] for w in fdist.items() if w[0] not in stopwords][:n]    \n",
    "    return top_n_words\n",
    "\n",
    "def get_ngrams(n_gram, words, freq, n_best, stopwords):   \n",
    "    # Get all Bigrams/Trigrams for input words\n",
    "    \n",
    "    # Input\n",
    "    # n_gram: 2 (Bigram) or 3 (Trigram)\n",
    "    # words: tokenized words\n",
    "    # freq: Minimum number of occurances to count as an n-gram\n",
    "    # n_best: Top N n-grams to return\n",
    "    # stopwords: List of stopwords\n",
    "    \n",
    "    # Output\n",
    "    # collocations: List of Top N n-gram tuples\n",
    "    \n",
    "    finder = None\n",
    "    scorer = None\n",
    "    if n_gram == 2:\n",
    "        finder = nltk.BigramCollocationFinder.from_words(words)\n",
    "        scorer = nltk.metrics.BigramAssocMeasures.jaccard\n",
    "    elif n_gram == 3:\n",
    "        finder = nltk.TrigramCollocationFinder.from_words(words)\n",
    "        scorer = nltk.metrics.TrigramAssocMeasures.jaccard\n",
    "    else:\n",
    "        raise Exception('Only Bigrams and Trigrams are supported.')\n",
    "    \n",
    "    finder.apply_freq_filter(freq)  # Minimum number of occurances\n",
    "    finder.apply_word_filter(lambda w: w in stopwords)\n",
    "    collocations = finder.nbest(scorer, n_best)\n",
    "    return collocations\n",
    "\n",
    "def test_ds_words():\n",
    "    \n",
    "    # Reads the description and job position text from a file of job posts,\n",
    "    # tokenizing the results to view top n words, bigrams, and trigrams. Results\n",
    "    # are written to a text file. \n",
    "   \n",
    "    job_data = read_pickle('Job Data.pkl')\n",
    "    \n",
    "    # Tokenize the job text\n",
    "    summ_words = []\n",
    "    desr_words = []    \n",
    "    for _, row in job_data.iterrows():\n",
    "        if row['position'] is not None:\n",
    "            summ_words += get_word_tokenize(row['position'])    \n",
    "        if row['description'] is not None:\n",
    "            desr_words += get_word_tokenize(row['description']) \n",
    "      \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords += [',', '.', ':', '(', ')', '-', ';', '&', '!', '?','s']\n",
    "    \n",
    "    words_file = open(\"top_N_words.log\",\"w\")\n",
    "      \n",
    "    # Get the Top N words\n",
    "    top_n_summ_words = get_top_n_words(summ_words, 10, stopwords)\n",
    "    top_n_desr_words = get_top_n_words(desr_words, 50, stopwords) \n",
    "    \n",
    "    print >> words_file, 'Top 10 Job Position Wordsn'    \n",
    "                     \n",
    "    for top_word in top_n_summ_words:\n",
    "        print >> words_file, top_word\n",
    "          \n",
    "    print >> words_file, 'nn'\n",
    "    \n",
    "    print >> words_file, 'Top 50 Job Description Wordsn'    \n",
    "          \n",
    "    for top_word in top_n_desr_words:\n",
    "        print >> words_file, top_word\n",
    "          \n",
    "    min_hits = int(len(job_data) * 0.05)\n",
    "    \n",
    "    print >> words_file, 'Top 50 Job Description Bigramsn'\n",
    "          \n",
    "    # Get the Bigrams\n",
    "    big_2_words = get_ngrams(2, desr_words, min_hits, 50, stopwords)\n",
    "      \n",
    "    print >> words_file, 'nn'\n",
    "          \n",
    "    for top_word in big_2_words:\n",
    "        print >> words_file, ' '.join(top_word)\n",
    "          \n",
    "    print >> words_file, 'nn'  \n",
    "    \n",
    "    print >> words_file, 'Top 50 Job Description Trigramsn'          \n",
    "               \n",
    "    # Get the Trigrams\n",
    "    big_3_words = get_ngrams(3, desr_words, min_hits, 50, stopwords)\n",
    "      \n",
    "    for top_word in big_3_words:\n",
    "        print >> words_file, ' '.join(top_word)            \n",
    "          \n",
    "    words_file.close()\n",
    "    \n",
    "def normalize(s):\n",
    "    # Normalizes + tokenizes input text + punctuation\n",
    "    # from http://streamhacker.com/2011/10/31/fuzzy-string-matching-python/\n",
    "    \n",
    "    # Input\n",
    "    # s: input text\n",
    "    \n",
    "    # Output\n",
    "    # string of cleaned text\n",
    "    \n",
    "    words = tokenize.wordpunct_tokenize(s.lower().strip())\n",
    "    return ' '.join([stemmer.stem(w) for w in words])\n",
    " \n",
    "def fuzzy_match(s1, s2):\n",
    "    # Calculates the edit distance between two normalized strings\n",
    "    # from http://streamhacker.com/2011/10/31/fuzzy-string-matching-python/\n",
    "    \n",
    "    # Input\n",
    "    # s1, s2: input text\n",
    "    \n",
    "    # Output\n",
    "    # edit distance between the two strings\n",
    "    \n",
    "    return nltk.metrics.edit_distance(normalize(s1), normalize(s2))\n",
    "\n",
    "def get_lnkin_code_name(company, field):\n",
    "    # Gets the code, name values from an input dict\n",
    "    \n",
    "    # Input\n",
    "    # company: dict\n",
    "    # field: key in dict\n",
    "    \n",
    "    # Output\n",
    "    # field_code: company[field]['code']\n",
    "    # field_name: company[field]['name']\n",
    "    \n",
    "    field_code = None\n",
    "    field_name = None    \n",
    "    if field in company:\n",
    "        field_code = company[field]['code']\n",
    "        field_name = company[field]['name']\n",
    "    return field_code, field_name\n",
    "\n",
    "def get_year(dt):\n",
    "    # Separates out the year from an input date\n",
    "    \n",
    "    # Input\n",
    "    # dt: date\n",
    "    \n",
    "    # Output\n",
    "    # year or None\n",
    "    \n",
    "    if dt is None:\n",
    "        return None\n",
    "    else:\n",
    "        return dt.date().year\n",
    "\n",
    "def get_month(dt):\n",
    "    # Separates out the month from an input date\n",
    "    \n",
    "    # Input\n",
    "    # dt: date\n",
    "    \n",
    "    # Output\n",
    "    # month or None\n",
    "        \n",
    "    if dt is None:\n",
    "        return None\n",
    "    else:\n",
    "        return dt.date().month\n",
    "    \n",
    "def get_word_count(text):\n",
    "    # Returns number of words in an input text\n",
    "    \n",
    "    # Input\n",
    "    # text: input text\n",
    "    \n",
    "    # Output\n",
    "    # Number of words\n",
    "    \n",
    "    if text is not None:\n",
    "        return len(text.split())\n",
    "    else:\n",
    "        return 0  \n",
    "    \n",
    "def get_char_count(text):\n",
    "    # Returns number of characters in an input text\n",
    "    \n",
    "    # Input\n",
    "    # text: input text\n",
    "    \n",
    "    # Output\n",
    "    # Number of characters\n",
    "    \n",
    "    if text is not None:\n",
    "        return sum(len(s) for s in text.split())\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_est_seniority_value(est_seniority):    \n",
    "    # Converts the estimated seniority into an integer value\n",
    "    \n",
    "    # Input\n",
    "    # est_seniority: text\n",
    "    \n",
    "    # Output\n",
    "    # Integer\n",
    "    \n",
    "    return {\n",
    "            'junior' : 1,\n",
    "            'default' : 2,\n",
    "            'senior' : 3\n",
    "            }.get(est_seniority, -1)\n",
    "            \n",
    "def get_emply_count_value(employee_count_code):    \n",
    "    # Converts the employee count code into an integer value,\n",
    "    # which is the midpoint of the count range\n",
    "    \n",
    "    # Input\n",
    "    # employee_count_code: char\n",
    "    \n",
    "    # Output\n",
    "    # Integer\n",
    "    \n",
    "    return {\n",
    "            'A' : 1,        # 1\n",
    "            'B' : 6,        # 2 - 10\n",
    "            'C' : 30,       # 11 - 50\n",
    "            'D' : 125,      # 51 - 200\n",
    "            'E' : 350,      # 201 - 500\n",
    "            'F' : 750,      # 501 - 1,000\n",
    "            'G' : 3000,     # 1,001 - 5,000\n",
    "            'H' : 7500,     # 5,001 - 10,000\n",
    "            'I' : 20000     # 10,000+\n",
    "            }.get(employee_count_code, -1)   \n",
    "            \n",
    "def get_cmpny_type_value(company_type_code):    \n",
    "    # Converts the company type code into an integer value\n",
    "    \n",
    "    # Input\n",
    "    # company_type_code: char\n",
    "    \n",
    "    # Output\n",
    "    # Integer\n",
    "    \n",
    "    return {\n",
    "            'C' : 1,    # Public Company\n",
    "            'D' : 2,    # Educational\n",
    "            'N' : 3,    # Non-Profit\n",
    "            'O' : 4,    # Self Owned\n",
    "            'P' : 5,    # Privately Held\n",
    "            'S' : 6     # Partnership\n",
    "            }.get(company_type_code, -1) \n",
    "\n",
    "def round_to_thousands(x):  \n",
    "    # Rounds normally to nearest thousand\n",
    "    \n",
    "    # Input\n",
    "    # x: integer\n",
    "    \n",
    "    # Output\n",
    "    # Rounded integer\n",
    "    \n",
    "    return int(math.floor(float(x) / 1000.0 + 0.5)) * 1000     \n",
    "        \n",
    "def update_company_data_from_linkedin():\n",
    "    \n",
    "    # Retrieves all of the company names from the job postings,\n",
    "    # and queries LinkedIn for additional information\n",
    "    \n",
    "    # Define CONSUMER_KEY, CONSUMER_SECRET,  \n",
    "    # USER_TOKEN, and USER_SECRET from the credentials \n",
    "    # provided in your LinkedIn application\n",
    "    \n",
    "    # Instantiate the developer authentication class\n",
    "    \n",
    "    authentication = linkedin.LinkedInDeveloperAuthentication(LINKEDIN_CONSUMER_KEY, LINKEDIN_CONSUMER_SECRET, \n",
    "                                                              LINKEDIN_OAUTH_USER_TOKEN, LINKEDIN_OAUTH_USER_SECRET, \n",
    "                                                              RETURN_URL, linkedin.PERMISSIONS.enums.values())\n",
    "    \n",
    "    # Pass it in to the app...\n",
    "    \n",
    "    application = linkedin.LinkedInApplication(authentication)    \n",
    "    \n",
    "    job_data = read_pickle('Job Data.pkl')\n",
    "    company_list = np.unique(job_data.name.values.ravel())\n",
    "        \n",
    "    # Set dict of return values and inputs\n",
    "    comp_sels = [{'companies': ['name', 'universal-name', 'description', 'company-type', 'industries', 'status', 'employee-count-range', 'specialties', 'website-url']}]\n",
    "    comp_params = {'keywords' : None}\n",
    "    \n",
    "    # Data dictionaries - going to convert them into Pandas dataframes\n",
    "    linkedin_companies = {}\n",
    "    linkedin_industries = {}\n",
    "    linkedin_specialities = {}\n",
    "    \n",
    "    # Loop through the unique set of companies\n",
    "    for idx, comp_name in enumerate(company_list):\n",
    "        comp_params['keywords'] = comp_name # Set company name as keyword       \n",
    "        comp_vals = application.search_company(selectors = comp_sels, params = comp_params)\n",
    "        \n",
    "        if comp_vals['companies']['_total'] == 0:   # No results returned\n",
    "            continue\n",
    "        \n",
    "        # Calculate the edit distance between the returned results and the input name\n",
    "        dist_vals = []        \n",
    "        for jdx, company in enumerate(comp_vals['companies']['values']):\n",
    "            link_comp_name = company['name']\n",
    "            name_dist = fuzzy_match(comp_name, link_comp_name)\n",
    "            dist_vals.append([link_comp_name, name_dist, jdx])\n",
    "            \n",
    "        # Sort the values and choose the best one\n",
    "        sort_dist_vals = sorted(dist_vals, key=lambda s: s[1])\n",
    "        best_guess_company = comp_vals['companies']['values'][sort_dist_vals[0][2]]\n",
    "        best_guess_name = sort_dist_vals[0][0]\n",
    "        \n",
    "        status_code, status_name = get_lnkin_code_name(best_guess_company, 'status')\n",
    "        company_type_code, company_type_name = get_lnkin_code_name(best_guess_company, 'companyType')\n",
    "        employee_count_code, employee_count_name = get_lnkin_code_name(best_guess_company, 'employeeCountRange')\n",
    "        \n",
    "        # Store company related data in a dictionary\n",
    "        linkedin_company = {}\n",
    "        linkedin_company['name'] = comp_name        \n",
    "        linkedin_company['lnkn_name'] = best_guess_name        \n",
    "        linkedin_company['lnkn_universal_name'] = best_guess_company.get('universalName')\n",
    "        linkedin_company['lnkn_description'] = best_guess_company.get('description')\n",
    "        linkedin_company['status_code'] = status_code\n",
    "        linkedin_company['status_name'] = status_name\n",
    "        linkedin_company['company_type_code'] = company_type_code\n",
    "        linkedin_company['company_type_name'] = company_type_name\n",
    "        linkedin_company['employee_count_code'] = employee_count_code\n",
    "        linkedin_company['employee_count_name'] = employee_count_name\n",
    "        linkedin_company['websiteUrl'] = best_guess_company.get('websiteUrl')                \n",
    "        linkedin_companies[idx] = linkedin_company\n",
    "                        \n",
    "        # Store industry data in a separate dict\n",
    "        if 'industries' in best_guess_company:\n",
    "            if best_guess_company['industries']['_total'] > 0:\n",
    "                ind_start = len(linkedin_industries)\n",
    "                for jdx, industry in enumerate(best_guess_company['industries']['values']):\n",
    "                    linkedin_industry = {}\n",
    "                    linkedin_industry['lnkn_name'] = best_guess_name\n",
    "                    linkedin_industry['industry_type_code'] = industry['code']\n",
    "                    linkedin_industry['industry_type_name'] = industry['name']\n",
    "                    linkedin_industries[ind_start + jdx] = linkedin_industry\n",
    "                \n",
    "        # Store speciality data in a separate dict\n",
    "        if 'specialties' in best_guess_company:\n",
    "            if best_guess_company['specialties']['_total'] > 0:\n",
    "                spec_start = len(linkedin_specialities)\n",
    "                for jdx, speciality in enumerate(best_guess_company['specialties']['values']):\n",
    "                    linkedin_speciality = {}\n",
    "                    linkedin_speciality['lnkn_name'] = best_guess_name\n",
    "                    linkedin_speciality['speciality'] = speciality\n",
    "                    linkedin_specialities[spec_start + jdx] = linkedin_speciality                \n",
    "    \n",
    "    # Convert to Pandas dataframes\n",
    "    company_data = pd.DataFrame.from_dict(linkedin_companies, orient='index')\n",
    "    industry_data = pd.DataFrame.from_dict(linkedin_industries, orient='index')\n",
    "    speciality_data = pd.DataFrame.from_dict(linkedin_specialities, orient='index')\n",
    "    \n",
    "    # Pickle and write to spreadsheets\n",
    "    company_data.to_pickle('LinkedIn Company Data.pkl')\n",
    "    industry_data.to_pickle('LinkedIn Industry Data.pkl')\n",
    "    speciality_data.to_pickle('LinkedIn Speciality Data.pkl')\n",
    "    \n",
    "    wrtr = pd.ExcelWriter('LinkedIn Data.xlsx')\n",
    "    company_data.to_excel(wrtr, 'Companies')\n",
    "    industry_data.to_excel(wrtr, 'Industries')\n",
    "    speciality_data.to_excel(wrtr, 'Specialities')   \n",
    "    wrtr.save()\n",
    "    \n",
    "    # Grab some simple statistics from the data generated and write it to a spreadsheet\n",
    "    # for followup analysis\n",
    "    \n",
    "    employee_count = pd.DataFrame(company_data.groupby(['employee_count_name']).size())\n",
    "    company_type = pd.DataFrame(company_data.groupby(['company_type_name']).size())    \n",
    "    industry_count = pd.DataFrame(industry_data.groupby(['industry_type_name']).size())\n",
    "    speciality_count = pd.DataFrame(speciality_data.groupby(['speciality']).size())\n",
    "    \n",
    "    wrtr = pd.ExcelWriter('LinkedIn Data Stats.xlsx')\n",
    "    employee_count.to_excel(wrtr, 'Employee Count')\n",
    "    company_type.to_excel(wrtr, 'Company Type')\n",
    "    industry_count.to_excel(wrtr, 'Industry Count')\n",
    "    speciality_count.to_excel(wrtr, 'Speciality Count')\n",
    "    wrtr.save()\n",
    "    \n",
    "def prepare_and_merge_data():\n",
    "    \n",
    "    # Retrieves all dataframes and merges into a single dataframe\n",
    "    # which is then pickled\n",
    "    \n",
    "    job_data = read_pickle('Job Data.pkl')\n",
    "    company_data = read_pickle('LinkedIn Company Data.pkl')\n",
    "    industry_data = read_pickle('LinkedIn Industry Data.pkl')\n",
    "    speciality_data = read_pickle('LinkedIn Speciality Data.pkl')    \n",
    "    \n",
    "    # Add in derived data and fill in blank data\n",
    "        \n",
    "    job_data['post_year'] = job_data.date_posted.apply(get_year)    # Get date_posted year\n",
    "    job_data['post_month'] = job_data.date_posted.apply(get_month)  # Get date_posted month\n",
    "    job_data['desc_word_count'] = job_data.description.apply(get_word_count)    # Number of words in job description\n",
    "    job_data['desc_char_count'] = job_data.description.apply(get_char_count)    # Number of characters in job description\n",
    "    job_data['estimated_seniority_value'] = job_data.estimated_seniority.apply(get_est_seniority_value) # Convert estimated seniority to an integer\n",
    "        \n",
    "    company_data.loc[company_data.employee_count_code.isnull(), 'employee_count_code'] = 'D'    # '51-200'\n",
    "    company_data.loc[company_data.company_type_code.isnull(), 'company_type_code'] = 'P'    # 'Privately Held'        \n",
    "    company_data['employee_count_value'] = company_data.employee_count_code.apply(get_emply_count_value) # Convert employee count code to an integer\n",
    "    company_data['company_type_value'] = company_data.company_type_code.apply(get_cmpny_type_value) # Convert company type code to an integer\n",
    "    \n",
    "    industry_data = pd.merge(industry_data, company_data[['lnkn_name']], how = 'right', on = 'lnkn_name')\n",
    "    industry_data.loc[industry_data.industry_type_name.isnull(), 'industry_type_name'] = 'Unknown'\n",
    "            \n",
    "    # Converting the Industry and Speciality data into dataframes of frequencies\n",
    "    # Only counting a subset of specialities as data science-y\n",
    "    industry_group = industry_data[['lnkn_name', 'industry_type_name']].groupby(['lnkn_name', 'industry_type_name']).size().unstack('industry_type_name')        \n",
    "    industry_group[industry_group.notnull()] = 1\n",
    "    industry_group[industry_group.isnull()] = 0\n",
    "        \n",
    "    ds_specialities = ['Big Data', 'Analytics', 'Machine Learning', 'analytics', 'Data Science']\n",
    "    ds_specialities.extend(['Big Data Analytics', 'Natural Language Processing', 'Predictive Analytics', 'Data Mining'])\n",
    "    speciality_group = speciality_data[speciality_data.speciality.isin(ds_specialities)].groupby(['lnkn_name', 'speciality']).size().unstack('speciality')    \n",
    "    speciality_group = pd.merge(speciality_group, company_data[['lnkn_name']], how = 'right', right_on = 'lnkn_name', left_index = True)   \n",
    "    speciality_group.set_index('lnkn_name', inplace = True)\n",
    "    speciality_group[speciality_group.notnull()] = 1\n",
    "    speciality_group[speciality_group.isnull()] = 0\n",
    "        \n",
    "    # Merge the dataframes\n",
    "    merge_data = pd.merge(job_data, company_data, on = 'name') \n",
    "    merge_data = pd.merge(merge_data, industry_group, left_on = 'lnkn_name', right_index = True)\n",
    "    merge_data = pd.merge(merge_data, speciality_group, how = 'left', left_on = 'lnkn_name', right_index = True)\n",
    "        \n",
    "    merge_data.to_pickle('Clean Job Data.pkl')\n",
    "\n",
    "def print_histogram(y, num_bins):   \n",
    "    # Prints a histogram of input array with equally spaced bins\n",
    "    \n",
    "    # Input\n",
    "    # y: array\n",
    "    # num_bins: number of bins in histogram\n",
    "    \n",
    "    _, _, patches = py.hist(y, num_bins, histtype='stepfilled')\n",
    "    py.setp(patches, 'facecolor', 'g', 'alpha', 0.75)\n",
    "    py.show()\n",
    "\n",
    "def get_train_test_sets(x, y, is_small_set):\n",
    "    # Routine to consolidate the train/test set retrieval\n",
    "    \n",
    "    # Input\n",
    "    # x, y: input arrays\n",
    "    # is_small_set: whether the input sets are small (and thus we use a more even train/test split)\n",
    "    \n",
    "    # Output\n",
    "    # split training, test sets \n",
    "    \n",
    "    if is_small_set:\n",
    "        return cv.train_test_split(x, y, train_size = 0.5, random_state = 0) \n",
    "    else:\n",
    "        return cv.train_test_split(x, y, train_size = 0.8, random_state = 0)\n",
    "    \n",
    "def convert_to_salary_range(salary_value):\n",
    "    # Groups salary amounts into ranges of 10k\n",
    "    \n",
    "    # Input\n",
    "    # salary_value: salary amount\n",
    "    \n",
    "    # Output\n",
    "    # Salary range value\n",
    "    \n",
    "    return int(math.floor(float(salary_value) / 10000.0 + 0.5))\n",
    "    \n",
    "def get_header_style():\n",
    "    # Creates style object for xlwt spreadsheet\n",
    "    # Font style - bold and underline\n",
    "    \n",
    "    header_font = xlwt.Font()\n",
    "    header_font.bold = True\n",
    "    header_font.underline = True    \n",
    "    \n",
    "    header_style = xlwt.XFStyle()\n",
    "    header_style.font = header_font\n",
    "        \n",
    "    return header_style\n",
    "    \n",
    "def get_grid_search_values(model, grid_params, x_train, y_train, x_test, y_test, scoring_criteria = 'mean_squared_error'):  \n",
    "    # Run a grid search on a model, and return the train / test score and MSE on the best result\n",
    "    \n",
    "    # Input\n",
    "    # model: scikit-learn model\n",
    "    # grid_params: dict of parameter space\n",
    "    # x_train: independent variables training set\n",
    "    # y_train: dependent variable training set\n",
    "    # x_test: independent variables test set\n",
    "    # y_test: dependent variable test set\n",
    "    # scoring_criteria: model scoring criteria\n",
    "    \n",
    "    # Output\n",
    "    # best_model: model that produced the best results\n",
    "    # para_search.best_params_: best grid parameters\n",
    "    # train_score: training score\n",
    "    # test_score: test score\n",
    "    # train_mse: training mse\n",
    "    # test_mse: test mse\n",
    "    \n",
    "    para_search = grid_search.GridSearchCV(model, grid_params, scoring = scoring_criteria, cv = 5).fit(x_train, y_train)\n",
    "    best_model = para_search.best_estimator_\n",
    "    train_score = best_model.score(x_train, y_train)\n",
    "    test_score = best_model.score(x_test, y_test)\n",
    "    train_mse = metrics.mean_squared_error(best_model.predict(x_train), y_train)\n",
    "    test_mse = metrics.mean_squared_error(best_model.predict(x_test), y_test)\n",
    "    \n",
    "    return best_model, para_search.best_params_, train_score, test_score, train_mse, test_mse \n",
    "\n",
    "def get_model_values(model, x_train, y_train, x_test, y_test):\n",
    "    # Fit a model and return the score and mse\n",
    "    \n",
    "    # Input\n",
    "    # model: scikit-learn model\n",
    "    # x_train: independent variables training set\n",
    "    # y_train: dependent variable training set\n",
    "    # x_test: independent variables test set\n",
    "    # y_test: dependent variable test set\n",
    "    \n",
    "    # Output\n",
    "    # train_score: training score\n",
    "    # test_score: test score\n",
    "    # train_mse: training mse\n",
    "    # test_mse: test mse        \n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    train_score = model.score(x_train, y_train)\n",
    "    test_score = model.score(x_test, y_test)\n",
    "    train_mse = metrics.mean_squared_error(model.predict(x_train), y_train)\n",
    "    test_mse = metrics.mean_squared_error(model.predict(x_test), y_test)\n",
    "    \n",
    "    return train_score, test_score, train_mse, test_mse\n",
    "\n",
    "def get_best_k_model(model, max_k, x, y):\n",
    "    # Fit a model using a range of best-k values, \n",
    "    # returning the model that produces the best test score\n",
    "    \n",
    "    # Input\n",
    "    # model: scikit-learn model\n",
    "    # max_k: maximum k-value to iterate to (inclusive)\n",
    "    # x: independent variables\n",
    "    # y: dependent variable\n",
    "    \n",
    "    # Output\n",
    "    # best_k: Number of dependent variables using to produce output\n",
    "    # train_score: training score\n",
    "    # test_score: test score\n",
    "    # train_mse: training mse\n",
    "    # test_mse: test mse       \n",
    "    \n",
    "    test_scores = []\n",
    "    k_vals = []    \n",
    "    \n",
    "    k_limit = min(max_k, len(x.columns))\n",
    "    for k_val in range(1, k_limit + 1):\n",
    "        best_x = fs.SelectKBest(fs.chi2, k = k_val).fit_transform(x, y)\n",
    "        x_train, x_test, y_train, y_test = cv.train_test_split(best_x, y, test_size = 0.2, random_state = 0)\n",
    "        test_scores.append(model.fit(x_train, y_train).score(x_test, y_test))\n",
    "        k_vals.append(k_val)\n",
    "\n",
    "    best_k = k_vals[np.argmax(test_scores)]\n",
    "    best_x = fs.SelectKBest(fs.chi2, k = best_k).fit_transform(x, y)\n",
    "    x_train, x_test, y_train, y_test = cv.train_test_split(best_x, y, test_size = 0.2, random_state = 0)\n",
    "       \n",
    "    train_score, test_score, train_mse, test_mse = get_model_values(model, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    return best_k, train_score, test_score, train_mse, test_mse\n",
    "\n",
    "def get_clean_column_names(job_data, drop_cols):    \n",
    "    # Configure the final set of columns and remove any columns with flat values\n",
    "    \n",
    "    # Input\n",
    "    # job_data: Pandas dataframe\n",
    "    # drop_cols: columns we're excluding\n",
    "    \n",
    "    # Output\n",
    "    # data_cols: non-excluded columns that have non-flat values\n",
    "    \n",
    "    data_cols = []\n",
    "    for col in job_data.columns.values.tolist():\n",
    "        if col in drop_cols:\n",
    "            continue\n",
    "        col_vals = job_data[col].values.ravel()\n",
    "        if np.min(col_vals) != np.max(col_vals):\n",
    "            data_cols.append(col)\n",
    "            \n",
    "    return data_cols\n",
    "\n",
    "def normalize_and_apply_pca(job_data, pca):\n",
    "    # Normalize a series of job data and then apply PCA\n",
    "    \n",
    "    # Input\n",
    "    # job_data: Pandas dataframe\n",
    "    # pca: scikit-learn PCA object\n",
    "    \n",
    "    # Output\n",
    "    # transformed normalized data\n",
    "    \n",
    "    norm_data = (job_data - job_data.mean()) / job_data.std()\n",
    "    return pca.fit_transform(norm_data)\n",
    "\n",
    "def write_to_spreadsheet(model_name, dataset_name, train_score, test_score, train_mse, test_mse, best_k, best_params, sh, row):\n",
    "    # Write a set of data to an xlwt spreadsheet\n",
    "    \n",
    "    # Input\n",
    "    # model_name: Name of model\n",
    "    # dataset_name: Name of dataset\n",
    "    # train_score: Training score\n",
    "    # test_score: Test score\n",
    "    # train_mse: Training MSE\n",
    "    # test_mse: Test MSE\n",
    "    # best_k: Best-K value, if we used Best-K for modeling\n",
    "    # best_params: Best Grid parameters, if we used grid modeling\n",
    "    # sh: xlwt spreadsheet\n",
    "    # row: row number\n",
    "    \n",
    "    # Output\n",
    "    # updates the row number\n",
    "\n",
    "    sh.write(row, 0, model_name)\n",
    "    sh.write(row, 1, dataset_name)\n",
    "    sh.write(row, 2, train_score)\n",
    "    sh.write(row, 3, test_score)\n",
    "    sh.write(row, 4, train_mse)\n",
    "    sh.write(row, 5, test_mse)\n",
    "    if best_k is not None:\n",
    "        sh.write(row, 6, best_k)\n",
    "    if best_params is not None:\n",
    "        sh.write(row, 7, str(best_params))\n",
    "    \n",
    "    return row + 1\n",
    "    \n",
    "def run_model():    \n",
    "    # Run the models against the data and write the results to a spreadsheet\n",
    "    \n",
    "    # Retrieve the data that we processed in prepare_and_merge_data()\n",
    "    clean_job_data = read_pickle('Clean Job Data.pkl')\n",
    "    clean_job_data = clean_job_data[clean_job_data.estimated_salary > 0.0]\n",
    "        \n",
    "    # These are the columns that we don't need\n",
    "    drop_cols = ['company_id', 'date_posted', 'description', 'estimated_salary', 'expiration_date']\n",
    "    drop_cols.extend(['job_id', 'pay_rate', 'position', 'skill_count', 'source_uri', 'estimated_seniority'])\n",
    "    drop_cols.extend(['name', 'company_industry', 'company_type', 'is_public', 'number_of_employees', 'status_name'])\n",
    "    drop_cols.extend(['status_code', 'lnkn_description', 'websiteUrl', 'employee_count_code', 'lnkn_universal_name'])\n",
    "    drop_cols.extend(['company_type_name', 'lnkn_name', 'employee_count_name', 'company_type_code', 'clean_pay_rate_annualized'])\n",
    "    \n",
    "    # Use records that have the pay rate provided in the job post - this is a small set\n",
    "    pay_rate_data = clean_job_data[clean_job_data.clean_pay_rate_annualized.notnull()]\n",
    "    pay_cols = get_clean_column_names(pay_rate_data, drop_cols)\n",
    "    #print 'Number of Clean Pay Rate Records: {}'.format(len(pay_rate_data))                 \n",
    "    x1 = pay_rate_data[pay_cols].astype(int)\n",
    "    y1 = pay_rate_data.clean_pay_rate_annualized\n",
    "    _, _, y1_train, y1_test = get_train_test_sets(x1, y1, True)\n",
    "#    print '{} Training Records / {} Testing Records'.format(y1_train.size, y1_test.size)\n",
    "    \n",
    "    # Use records that have an estimated salary, which we will round to nearest 1k\n",
    "#    print 'Number of Estimated Salary Records: {}'.format(len(clean_job_data))\n",
    "    est_sal_cols = get_clean_column_names(clean_job_data, drop_cols)\n",
    "    x2 = clean_job_data[est_sal_cols].astype(int)\n",
    "    y2 = clean_job_data.estimated_salary.apply(round_to_thousands) \n",
    "    _, _, y2_train, y2_test = get_train_test_sets(x2, y2, False)\n",
    "#    print '{} Training Records / {} Testing Records'.format(y2_train.size, y2_test.size)\n",
    "    \n",
    "    # Different approach - groups salaries in amounts of 10k and see if we can get better results\n",
    "    y3 = pay_rate_data.clean_pay_rate_annualized.apply(convert_to_salary_range) # Convert Pay Rate to a range\n",
    "    y4 = clean_job_data.estimated_salary.apply(convert_to_salary_range) # Convert Est Salary to a range\n",
    "    \n",
    "    # Transform the independent variables using PCA to see if that helps on some of the models\n",
    "    pca = PCA().set_params(n_components = 0.9)     \n",
    "    x3 = normalize_and_apply_pca(x1, pca)\n",
    "    x4 = normalize_and_apply_pca(x2, pca)\n",
    "                \n",
    "    results_book = xlwt.Workbook()    \n",
    "    head_style = get_header_style()\n",
    "    \n",
    "    pyrt_sh = results_book.add_sheet('Pay Rate')\n",
    "    pyrt_sh.write(0, 0, \"Model Name\", head_style)\n",
    "    pyrt_sh.write(0, 1, \"Dataset\", head_style)\n",
    "    pyrt_sh.write(0, 2, \"Training Score\", head_style)\n",
    "    pyrt_sh.write(0, 3, \"Testing Score\", head_style)\n",
    "    pyrt_sh.write(0, 4, \"Training MSE\", head_style)\n",
    "    pyrt_sh.write(0, 5, \"Testing MSE\", head_style)\n",
    "    pyrt_sh.write(0, 6, \"Best K\", head_style)\n",
    "    pyrt_sh.write(0, 7, \"Best Parameters\", head_style)\n",
    "    \n",
    "    estsal_sh = results_book.add_sheet('Est Salary')\n",
    "    estsal_sh.write(0, 0, \"Model Name\", head_style)\n",
    "    estsal_sh.write(0, 1, \"Dataset\", head_style)\n",
    "    estsal_sh.write(0, 2, \"Training Score\", head_style)\n",
    "    estsal_sh.write(0, 3, \"Testing Score\", head_style)\n",
    "    estsal_sh.write(0, 4, \"Training MSE\", head_style)\n",
    "    estsal_sh.write(0, 5, \"Testing MSE\", head_style)\n",
    "    estsal_sh.write(0, 6, \"Best K\", head_style)\n",
    "    estsal_sh.write(0, 7, \"Best Parameters\", head_style)    \n",
    "    \n",
    "    # Do an initial test using linear models with different shapes for the dependent variable\n",
    "    linear_datasets = [(\"Pay Rate\", x1, y1, True),\n",
    "                (\"Log Pay Rate\", x1, np.log(y1), True),\n",
    "                (\"Sqrt Pay Rate\", x1, np.sqrt(y1), True),\n",
    "                (\"Est Salary\", x2, y2, False),\n",
    "                (\"Log Est Salary\", x2, np.log(y2), False),\n",
    "                (\"Sqrt Est Salary\", x2, np.sqrt(y2), False),\n",
    "                (\"Pay Rate Range\", x1, y3, True),\n",
    "                (\"Log Pay Rate Range\", x1, np.log(y3), True),\n",
    "                (\"Sqrt Pay Rate Range\", x1, np.sqrt(y3), True),\n",
    "                (\"Est Salary Range\", x2, y4, False),\n",
    "                (\"Log Est Salary Range\", x2, np.log(y4), False),\n",
    "                (\"Sqrt Est Salary Range\", x2, np.sqrt(y4), False)                \n",
    "                ]\n",
    "         \n",
    "    linear_models = [(\"OLS\", linear_model.LinearRegression()),\n",
    "              (\"Ridge\", linear_model.RidgeCV(normalize = True, fit_intercept = False, scoring = 'mean_squared_error', cv = 5)),\n",
    "              (\"Lasso\", linear_model.LassoCV(normalize = True, fit_intercept = False, cv = 5))]\n",
    "    \n",
    "    prow = 1\n",
    "    erow = 1\n",
    "    for data in linear_datasets:\n",
    "        x_train, x_test, y_train, y_test = get_train_test_sets(data[1], data[2], data[3])\n",
    "          \n",
    "        for model in linear_models:            \n",
    "            train_score, test_score, train_mse, test_mse = get_model_values(model[1], x_train, y_train, x_test, y_test)\n",
    "            \n",
    "            if data[3] == True:\n",
    "                prow = write_to_spreadsheet(model[0], data[0], train_score, test_score, train_mse, test_mse, None, None, pyrt_sh, prow)\n",
    "            else:\n",
    "                erow = write_to_spreadsheet(model[0], data[0], train_score, test_score, train_mse, test_mse, None, None, estsal_sh, erow)\n",
    "       \n",
    "    # Test on a different set of models, where we're applying PCA to reduce the number of features        \n",
    "    datasets = [(\"Pay Rate\", x1, y1, True),\n",
    "                (\"PCA Pay Rate\", x3, y1, True),\n",
    "                (\"Pay Rate Range\", x1, y3, True),\n",
    "                (\"PCA Pay Rate Range\", x3, y3, True),\n",
    "                (\"Est Salary\", x2, y2, False),\n",
    "                (\"PCA Est Salary\", x4, y2, False),\n",
    "                (\"Est Salary Range\", x2, y4, False),\n",
    "                (\"PCA Est Salary Range\", x4, y4, False)             \n",
    "                ]\n",
    "    \n",
    "    models = [(\"KNN\", neighbors.KNeighborsClassifier(), {'n_neighbors' : np.arange(3, 9), 'weights' : ['uniform', 'distance'], 'p' : [1, 2]}),\n",
    "              (\"Decision Tree\", tree.DecisionTreeClassifier(), {'criterion' : ['gini', 'entropy'], 'max_features' : [None, 'auto', 'log2']}),\n",
    "              (\"Random Forest\", ensemble.RandomForestClassifier(), {'criterion': ['gini', 'entropy'], 'max_features' : [None, 'auto', 'log2'], 'n_estimators': np.arange(10, 110, 10)})\n",
    "              ]\n",
    "    \n",
    "    for data in datasets:         \n",
    "        x_train, x_test, y_train, y_test = get_train_test_sets(data[1], data[2], data[3])    \n",
    "     \n",
    "        for model in models:\n",
    "            _, best_params, train_score, test_score, train_mse, test_mse = get_grid_search_values(model[1], model[2], x_train, y_train, x_test, y_test, 'accuracy')                 \n",
    "            \n",
    "            if data[3] == True:\n",
    "                prow = write_to_spreadsheet(model[0], data[0], train_score, test_score, train_mse, test_mse, None, best_params, pyrt_sh, prow)\n",
    "            else:\n",
    "                erow = write_to_spreadsheet(model[0], data[0], train_score, test_score, train_mse, test_mse, None, best_params, estsal_sh, erow)            \n",
    "\n",
    "    # Use the best K on LDA - had collinearity issues with full feature set              \n",
    "    datasets = [(\"Pay Rate Range Best K\", x1, y3.values.ravel(), True),\n",
    "                (\"Est Salary Range Best K\", x2, y4.values.ravel(), False)              \n",
    "                ]\n",
    "    \n",
    "    models = [(\"LDA\", lda.LDA())]\n",
    "    \n",
    "    for data in datasets:         \n",
    "        for model in models: \n",
    "            best_k, train_score, test_score, train_mse, test_mse = get_best_k_model(model[1], 20, data[1], data[2])                            \n",
    "            \n",
    "            if data[3] == True:\n",
    "                prow = write_to_spreadsheet(model[0], data[0], train_score, test_score, train_mse, test_mse, best_k, None, pyrt_sh, prow)\n",
    "            else:\n",
    "                erow = write_to_spreadsheet(model[0], data[0], train_score, test_score, train_mse, test_mse, best_k, None, estsal_sh, erow)            \n",
    "                        \n",
    "    results_book.save(\"Model Results.xls\")         \n",
    "    \n",
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
